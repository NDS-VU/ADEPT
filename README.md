# ADEPT: Autoencoder with Differentially Expressed Genes and Imputation for a Robust Spatial Transcriptomics Clustering

## Introduction 

Recent advancements in spatial transcriptomics (ST) have enabled an in-depth understanding of complex tissue by allowing the measurement of gene expression at spots of tissue along with their spatial information. Several notable clustering methods have been introduced to utilize both spatial and transcriptional information in analysis of ST datasets. However, data quality across different ST sequencing techniques and types of datasets appears as a crucial factor that influences the performance of different methods and influences benchmarks. To harness both spatial context and transcriptional profile in ST data, we develop a novel graph-based multi-stage framework for robust clustering, called ADEPT. To control and stabilize data quality, ADEPT relies on selection of differentially expressed genes (DEGs) and imputation of the multiple DEG-based matrices for the initial and final clustering of a graph autoencoder backbone that minimizes the variance of clustering results. We benchmarked ADEPT against five other popular methods on ST data generated by different ST platforms. ADEPT demonstrated its robustness and superiority in different analyses such as spatial domain identification, visualization, spatial trajectory inference, and data denoising.

The general workflow of ADEPT works as follows:

![image](https:****.png)

## Installation
### Dependencies
scanpy==1.9.1

pytorch==1.8.0

pyG==2.0.1

pandas

numpy==1.20.3

scipy

matplolib

### Install from github
1. git clone --recursive https://github.com/maiziezhoulab/AquilaDeepFilter.git
2. conda create -n [EnvName] python=3.7
3. source activate [EnvName]
4. install pytorch (https://pytorch.org/get-started/locally/) + pytorch-geometric (https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html) before everything
5. pip install -r requirements.txt

## Usage

**test**

a. test

use **vcf2bed_training.py** and **vcf2bed_val.py** to generate .bed files before generating images

will have **3** .bed files as the output of **vcf2bed_training.py**

will have **1** .bed file as the output of **vcf2bed_val.py**

b. generate image from the .bed files (generated in last step) and .bam files (used for SV calling)

use **bed2image.py** to generate training/testing images. For training images, positive samples are generated from 2 .bed files corresponding to TP.vcf and FN.vcf while negative samples are generated from 1 .bed file corresponding to FP.vcf

c. augmentate the images generated in last step

use **augmentate.py** to augmentate the images


## Repo Structure and Output

1. The folder of AquilaDeepFilter, post and preprocess have corresponding scripts and codes for Running the AuilaDeepFilter software

2. The dependencies are documented in the requirements.txt.

3. The 'train' command in 'main.py' script will constantly store the weights for the training epoch with the best validation Acc. and stops after the convergence is reached.

4. The 'predict' command in 'main.py' script will generate output in the BED structure (but in .txt file format). It can be then converted back to vcf for evaluation.

5. The docker image of this project will also be uploaded later after all the configuration and testing work are done.

6. The uploaded weights for our model and the toy dataset could be found in zenodo: ...

Citation
--------
paper currently under review

